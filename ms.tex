\documentclass[letterpaper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage[utf8]{inputenc}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}

\newcommand{\hyperparams}{\boldsymbol{\alpha}}
\newcommand{\xx}{\boldsymbol{x}}

\title{What is MaxEnt?}
\author{Brendon J. Brewer}

\begin{document}
\maketitle



Relative entropy
\begin{eqnarray}
H(\boldsymbol{p}; \boldsymbol{q}) &=& -\sum_i p_i \log\left(\frac{p_i}{q_i}\right) 
\end{eqnarray}

\section{Testable Information}

\section{A Special Case: Expected Values}\label{sec:expectations}
Refer to Jaynes here...I don't want to present all the details.

\section{Where does testable information come from?}

Example about hearing ``s√≠/si'' as an answer to a question,
and inferring the language (Spanish or French).

The joint prior is:

\begin{eqnarray}
\begin{array}{|c|c|c|}
\hline
	&	S	& \bar{S}\\
\hline
H_1 & 0.25  & 0.25\\
\hline
H_2 & 0.05 & 0.45\\
\hline
\end{array}
\end{eqnarray}

The marginal probabilities for $H_1$ and $H_2$ are both 0.5, and the
marginal probabilities for $S$ and $\bar{S}$ are 0.3 and 0.7 respectively.


\section{MaxEnt and Bayesian Inference}\label{sec:bayes}

\section{MaxEnt and Hierarchical Bayesian Models}
Suppose we have a discrete prior distribution $q(\xx)$. Consider some
function of the unknown quantity, $f(\xx)$.
If the value of $f(\xx)$ is known, then our distribution for $\xx$ can
be updated from $q$ to $p$ as in Section~\ref{sec:bayes}.
If we receive testable information
about the expected value of $F$, then our distribution for $\xx$ can
be updated as in Section~\ref{sec:expectations}.

In this section we'll look at a different kind of testable information that
relates to $F$, but is not the expected value. What if we receive testable
information that asserts the marginal distribution for $F$?
For any distribution $p(\xx)$, the marginal distribution for $F$ is:
\begin{eqnarray}
p(F) &=& \sum_{\xx}\mathds{1}\left[f(\xx) = F\right]p(\xx)\label{eqn:constraint}
\end{eqnarray}
If $p(F)$ is a specified function, this is a large number of expected
value constraints on $p(\xx)$, one for each element of the hypothesis space
of possible $F$ values. Clearly, the specified marginal $p(F)$ cannot assign
non-zero probability to any possible $F$ values that have zero probability
under $q(\xx)$.

Since Equation~\ref{eqn:constraint} is really just a set of expectations,
the probability distribution
$p(\xx)$ with maximum entropy with respect to $q(\xx)$ that satisfies the
constraint~\ref{eqn:constraint} is:
\begin{eqnarray}
p(\xx) &=& \frac{q(\xx)}{Z}\exp\left\{-\sum_F \lambda_F \mathds{1}
\left[f(\xx) = F\right]  \right\}
\end{eqnarray}


\begin{thebibliography}{}
\bibitem[Caticha(2008)]{2008arXiv0808.0012C} Caticha, A.\ 2008.\ Lectures 
on Probability, Entropy, and Statistical Physics.\ ArXiv e-prints 
arXiv:0808.0012. 

\bibitem[Caticha and Giffin(2006)]{2006AIPC..872...31C} Caticha, A., 
Giffin, A.\ 2006.\ Updating Probabilities.\ Bayesian Inference and Maximum 
Entropy Methods In Science and Engineering 872, 31-42.

\bibitem[Giffin and Caticha(2007)]{2007AIPC..954...74G} Giffin, A., 
Caticha, A.\ 2007.\ Updating Probabilities with Data and Moments.\ Bayesian 
Inference and Maximum Entropy Methods in Science and Engineering 954, 
74-84.

\bibitem[Jaynes(1979)]{stand_on_entropy} Jaynes, E. T., 1979, ``Where do we Stand on Maximum Entropy?'' in The Maximum Entropy Formalism, R. D. Levine and M. Tribus (eds.), M. I. T. Press, Cambridge, MA, p. 15
\end{thebibliography}

\end{document}

