\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage[utf8]{inputenc}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}

\newcommand{\hyperparams}{\boldsymbol{\alpha}}
\newcommand{\xx}{\boldsymbol{x}}

\title{MaxEnt: What is it good for?}
\author{Brendon J. Brewer}

\begin{document}
\maketitle

\abstract{I discuss the principle of maximum (relative) entropy as a tool
for updating probability distributions. This principle is intuitively
compelling, as I will show through examples. However, it has been neglected
by most modern practitioners of Bayesian inference. I will argue that there
are two main reasons for this: i) some questionable applications
and discussions of the principle have appeared in the literature, and
ii) the situation which the principle seems designed to handle never seems
to arise in practical applications.}

\section{Introduction}
The principle of maximum entropy (MaxEnt) is an idea in probability theory
that is usually associated with \citet{jaynes}, although it has appeared in
various guises in the work of others \citep[e.g.][]{gibbs, boltzmann, shannon}.

The principle may be used
to {\it assign} or {\it update} a
subjective\footnote{in the sense that the probability distribution is a model
of the state of uncertainty of an idealised rational agent, rather than
a {\it frequency distribution} such as a histogram of people's heights.}
probability distribution when you obtain a certain kind of information
called {\it testable information}. In this article I will focus on the
updating application, as the assigning application can be viewed as a special
case where the prior distribution happened to be
uniform\footnote{A uniform probability distribution, either over a discrete
hypothesis space, or a continuous one with respect to a continuous, may be
justified by a symmetry argument or simply asserted as in the
``subjective Bayesian'' approach.}.

The principle is very intuitively compelling
and there are some axiomatic arguments in its favour
\citep[e.g.][]{2010arXiv1008.4831K}. However, MaxEnt has also been widely
misapplied (even by some of its staunchest defenders), misunderstood,
and poorly defended (at times, even by Jaynes himself).
Consequently, most modern Bayesians pay little attention to the principle,
although there is a tendency among physicists to invoke it as a justification
for a prior distribution (be it uniform, exponential, gaussian, or whatever).
The purpose of this article is to explore the principle from the point of
view of a Bayesian practitioner, discuss some of the properties that
make MaxEnt compelling, and propose some ideas about how it could be used in
practice.

\section{Probability}
We start with a {\it hypothesis space} of mutually exclusive and
exhaustive propositions $\{A_1, A_2, ..., A_N\}$ which we will usually
think of as possible values of an unknown quantity $x$.
On the basis of background
information $I$ (hereafter suppressed), we know that one and only one of these
propositions is true (i.e. the true value of $x$ is in our set),
but we don't know which. We model states of
uncertainty by probability distributions over the hypothesis space,
such as $\{p_i\}_{i=1}^N$ where $P(A_i) = p_i$ is the probability of $A_i$.
The standard loose Bayesian notation would write this distribution simply
as $p(x)$.

Of course all of the $p_i$ must be non-negative and their sum must be 1.
From this probability distribution, various probabilities can be derived
using the sum and product rules. For example, the probability of
($A_4$ {\bf or} $A_5$) is $p_4 + p_5$, and the probability of $A_5$ given
($A_5$ {\bf or} $A_8$) is $p_5/(p_5 + p_8)$. The rules of probability must
be obeyed if the rules for combining are to remain consistent
with the symmetries of logical {\bf and} and {\bf or}
\citep{2010arXiv1008.4831K}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{distribution.pdf}
\caption{\label{fig:distribution}}
\end{center}
\end{figure}

The relative entropy of distribution $p$ from distribution $q$ is:
\begin{eqnarray}
H(p; q) &=& -\sum_i p_i \log\left(\frac{p_i}{q_i}\right) 
\end{eqnarray}
and we are to choose the distribution $p$ to maximise $H$ subject to the
constraints of normalisation and any other constraints.

\section{What is testable information?}


\section{A special case: updating given propositions}
Consider the following testable information:

\begin{eqnarray}
P(D) = 1
\end{eqnarray}
where $D$ is a proposition about the value of $x$. For example, consider
updating from the prior $q(x)$ shown in Figure~\ref{fig:distribution}, to
a posterior $p(x)$, using the testable information
\begin{eqnarray}
P(3 \leq x \leq 5) = 1.
\end{eqnarray}

Clearly, the distribution $q(x)$ doesn't satisfy this constraint, so we
need to update to a new distribution $p(x)$ using MaxEnt.




\section{A special case: expected values}\label{sec:expectations}
The most common kind of testable information analysed using MaxEnt is
expected values. Updating from a prior $\boldsymbol{q}$ to a posterior
$\boldsymbol{p}$ given an expected value constraint:

\begin{eqnarray}
\left<f(x)\right>_{\boldsymbol{p}} = \sum_{i=1}^N p_i f(x_i) = F.
\end{eqnarray}

The resulting posterior distribution given by MaxEnt is:

\begin{eqnarray}
p(x) &=& \frac{1}{Z}q(x)\exp\left[-\lambda f(x)\right] 
\end{eqnarray}

\section{Where is testable information in the real world?}

Example about hearing ``s√≠/si'' as an answer to a question,
and inferring the language (Spanish or French).

The joint prior is:

\begin{eqnarray}
\begin{array}{|c|c|c|}
\hline
	&	S	& \bar{S}\\
\hline
H_1 & 0.25  & 0.25\\
\hline
H_2 & 0.05 & 0.45\\
\hline
\end{array}
\end{eqnarray}

The marginal probabilities for $H_1$ and $H_2$ are both 0.5, and the
marginal probabilities for $S$ and $\bar{S}$ are 0.3 and 0.7 respectively.


\section{MaxEnt and Bayesian inference}\label{sec:bayes}

\section{MaxEnt and ``Hierarchical Bayesian'' models}
Suppose we have a discrete prior distribution $q(\xx)$. Consider some
function of the unknown quantity, $f(\xx)$.
If the value of $f(\xx)$ is known, then our distribution for $\xx$ can
be updated from $q$ to $p$ as in Section~\ref{sec:bayes}.
If we receive testable information
about the expected value of $F$, then our distribution for $\xx$ can
be updated as in Section~\ref{sec:expectations}.

In this section we'll look at a different kind of testable information that
relates to $F$, but is not the expected value. What if we receive testable
information that asserts the marginal distribution for $F$?
For any distribution $p(\xx)$, the marginal distribution for $F$ is:
\begin{eqnarray}
p(F) &=& \sum_{\xx}\mathds{1}\left[f(\xx) = F\right]p(\xx)\label{eqn:constraint}
\end{eqnarray}
If $p(F)$ is a specified function, this is a large number of expected
value constraints on $p(\xx)$, one for each element of the hypothesis space
of possible $F$ values. Clearly, the specified marginal $p(F)$ cannot assign
non-zero probability to any possible $F$ values that have zero probability
under $q(\xx)$.

Since Equation~\ref{eqn:constraint} is really just a set of expectations,
the probability distribution
$p(\xx)$ with maximum entropy with respect to $q(\xx)$ that satisfies the
constraint~\ref{eqn:constraint} is:
\begin{eqnarray}
p(\xx) &=& \frac{q(\xx)}{Z}\exp\left\{-\sum_F \lambda_F \mathds{1}
\left[f(\xx) = F\right]  \right\}
\end{eqnarray}


\section*{Acknowledgements}
It is a pleasure to thank
Ariel Caticha (Albany),
Adom Giffin (Clarkson)
John Skilling (MaxEnt Data Consultants),
Carlos Rodriguez (Albany),
Stephen Gull (Cambridge),
Kevin Knuth (Albany),
Iain Murray (Edinburgh),
Anna Pancoast (UCSB),
Geraint Lewis (Sydney),
David Hogg (NYU),
Tom Loredo (Cornell),
Daniela Huppenkothen (NYU),
Peter Tuthill (Sydney),


\begin{thebibliography}{}
\bibitem[Caticha(2008)]{2008arXiv0808.0012C} Caticha, A.\ 2008.\ Lectures 
on Probability, Entropy, and Statistical Physics.\ ArXiv e-prints 
arXiv:0808.0012. 

\bibitem[Caticha and Giffin(2006)]{2006AIPC..872...31C} Caticha, A., 
Giffin, A.\ 2006.\ Updating Probabilities.\ Bayesian Inference and Maximum 
Entropy Methods In Science and Engineering 872, 31-42.

\bibitem[Giffin and Caticha(2007)]{2007AIPC..954...74G} Giffin, A., 
Caticha, A.\ 2007.\ Updating Probabilities with Data and Moments.\ Bayesian 
Inference and Maximum Entropy Methods in Science and Engineering 954, 
74-84.

\bibitem[Jaynes(1979)]{stand_on_entropy} Jaynes, E. T., 1979, ``Where do we Stand on Maximum Entropy?'' in The Maximum Entropy Formalism, R. D. Levine and M. Tribus (eds.), M. I. T. Press, Cambridge, MA, p. 15

\bibitem[Jaynes(2003)]{jaynes} Jaynes, E. T., 2003, ``Probability Theory:
The Logic of Science'', ed. G.~Larry~Bretthorst, Cambridge University Press

\bibitem[Knuth and Skilling(2010)]{2010arXiv1008.4831K} Knuth, K.~H., 
Skilling, J.\ 2010.\ Foundations of Inference.\
Axioms 1.1 (2012): 38-73.\
ArXiv e-prints arXiv:1008.4831. 
\end{thebibliography}

\end{document}

