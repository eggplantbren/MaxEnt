\documentclass[letterpaper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage[utf8]{inputenc}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}

\newcommand{\hyperparams}{\boldsymbol{\alpha}}
\newcommand{\xx}{\boldsymbol{x}}

\title{MaxEnt: What is it good for?}
\author{Brendon J. Brewer}

\begin{document}
\maketitle

The principle of maximum entropy (MaxEnt) is an idea in probability theory
that is usually associated with \citet{jaynes}. The principle may be used
to {\it assign} or {\it update} a
subjective\footnote{in the sense that the probability distribution is a model
of the state of uncertainty of an idealised rational agent, rather than
a {\it frequency distribution} such as a histogram of people's heights.}
probability distribution when you obtain a certain kind of information
called {\it testable information}. The principle is very intuitively compelling
and there are some axiomatic arguments in its favour
\citep[e.g.][]{2010arXiv1008.4831K}. However, MaxEnt has also been widely
misapplied (even by some of its staunchest defenders), misunderstood,
and poorly defended (at times, even by Jaynes himself).
Consequently, most modern Bayesians pay little attention to the principle,
although there is a tendency among physicists to invoke it as a justification
for a prior distribution (be it uniform, exponential, gaussian, or whatever).

We start with a {\it hypothesis space} of mutually exclusive and
exhaustive propositions $A_1$, $A_2$, ..., $A_N$. One and only one of these
propositions is true, but we don't know which.



Relative entropy
\begin{eqnarray}
H(\boldsymbol{p}; \boldsymbol{q}) &=& -\sum_i p_i \log\left(\frac{p_i}{q_i}\right) 
\end{eqnarray}

\section{What is testable information?}


\section{A special case: updating given propositions}


\section{A special case: expected values}\label{sec:expectations}
Refer to Jaynes here...I don't want to present all the details.

\section{Where is testable information in the real world?}

Example about hearing ``s√≠/si'' as an answer to a question,
and inferring the language (Spanish or French).

The joint prior is:

\begin{eqnarray}
\begin{array}{|c|c|c|}
\hline
	&	S	& \bar{S}\\
\hline
H_1 & 0.25  & 0.25\\
\hline
H_2 & 0.05 & 0.45\\
\hline
\end{array}
\end{eqnarray}

The marginal probabilities for $H_1$ and $H_2$ are both 0.5, and the
marginal probabilities for $S$ and $\bar{S}$ are 0.3 and 0.7 respectively.


\section{MaxEnt and Bayesian inference}\label{sec:bayes}

\section{MaxEnt and ``Hierarchical Bayesian'' models}
Suppose we have a discrete prior distribution $q(\xx)$. Consider some
function of the unknown quantity, $f(\xx)$.
If the value of $f(\xx)$ is known, then our distribution for $\xx$ can
be updated from $q$ to $p$ as in Section~\ref{sec:bayes}.
If we receive testable information
about the expected value of $F$, then our distribution for $\xx$ can
be updated as in Section~\ref{sec:expectations}.

In this section we'll look at a different kind of testable information that
relates to $F$, but is not the expected value. What if we receive testable
information that asserts the marginal distribution for $F$?
For any distribution $p(\xx)$, the marginal distribution for $F$ is:
\begin{eqnarray}
p(F) &=& \sum_{\xx}\mathds{1}\left[f(\xx) = F\right]p(\xx)\label{eqn:constraint}
\end{eqnarray}
If $p(F)$ is a specified function, this is a large number of expected
value constraints on $p(\xx)$, one for each element of the hypothesis space
of possible $F$ values. Clearly, the specified marginal $p(F)$ cannot assign
non-zero probability to any possible $F$ values that have zero probability
under $q(\xx)$.

Since Equation~\ref{eqn:constraint} is really just a set of expectations,
the probability distribution
$p(\xx)$ with maximum entropy with respect to $q(\xx)$ that satisfies the
constraint~\ref{eqn:constraint} is:
\begin{eqnarray}
p(\xx) &=& \frac{q(\xx)}{Z}\exp\left\{-\sum_F \lambda_F \mathds{1}
\left[f(\xx) = F\right]  \right\}
\end{eqnarray}


\begin{thebibliography}{}
\bibitem[Caticha(2008)]{2008arXiv0808.0012C} Caticha, A.\ 2008.\ Lectures 
on Probability, Entropy, and Statistical Physics.\ ArXiv e-prints 
arXiv:0808.0012. 

\bibitem[Caticha and Giffin(2006)]{2006AIPC..872...31C} Caticha, A., 
Giffin, A.\ 2006.\ Updating Probabilities.\ Bayesian Inference and Maximum 
Entropy Methods In Science and Engineering 872, 31-42.

\bibitem[Giffin and Caticha(2007)]{2007AIPC..954...74G} Giffin, A., 
Caticha, A.\ 2007.\ Updating Probabilities with Data and Moments.\ Bayesian 
Inference and Maximum Entropy Methods in Science and Engineering 954, 
74-84.

\bibitem[Jaynes(1979)]{stand_on_entropy} Jaynes, E. T., 1979, ``Where do we Stand on Maximum Entropy?'' in The Maximum Entropy Formalism, R. D. Levine and M. Tribus (eds.), M. I. T. Press, Cambridge, MA, p. 15

\bibitem[Jaynes(2003)]{jaynes} Jaynes, E. T., 2003, ``Probability Theory:
The Logic of Science'', ed. G.~Larry~Bretthorst, Cambridge University Press

\bibitem[Knuth and Skilling(2010)]{2010arXiv1008.4831K} Knuth, K.~H., 
Skilling, J.\ 2010.\ Foundations of Inference.\
Axioms 1.1 (2012): 38-73.\
ArXiv e-prints arXiv:1008.4831. 
\end{thebibliography}

\end{document}

