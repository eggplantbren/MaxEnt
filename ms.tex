\documentclass[letterpaper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage[utf8]{inputenc}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}

\newcommand{\hyperparams}{\boldsymbol{\alpha}}
\newcommand{\xx}{\boldsymbol{x}}

\title{What is MaxEnt?}
\author{Brendon J. Brewer}

\begin{document}
\maketitle



Relative entropy
\begin{eqnarray}
H(\boldsymbol{p}; \boldsymbol{q}) &=& -\sum_i p_i \log\left(\frac{p_i}{q_i}\right) 
\end{eqnarray}

\section{Testable Information}

\section{A Special Case: Expected Values}\label{sec:expectations}
Refer to Jaynes here...I don't want to present all the details.

\section{Where does testable information come from?}

Example about hearing ``sÃ­/si'' as an answer to a question,
and inferring the language (Spanish or French).

The joint prior is:

\begin{eqnarray}
\begin{array}{|c|c|c|}
\hline
	&	S	& \bar{S}\\
\hline
H_1 & 0.25  & 0.25\\
\hline
H_2 & 0.05 & 0.45\\
\hline
\end{array}
\end{eqnarray}

The marginal probabilities for $H_1$ and $H_2$ are both 0.5, and the
marginal probabilities for $S$ and $\bar{S}$ are 0.3 and 0.7 respectively.


\section{MaxEnt and Bayesian Inference}\label{sec:bayes}

\section{MaxEnt and Hierarchical Bayesian Models}
Suppose we have a discrete prior distribution $q(\xx)$. Consider some
function of the unknown quantity, $f(\xx)$.
If the value of $F$ is known, then our distribution for $\xx$ can
be updated as in Section~\ref{sec:bayes}. If we receive testable information
about the expected value of $F$, then our distribution for $\xx$ can
be updated as in Section~\ref{sec:expectations}.

In this section we'll look at a different kind of testable information that
relates to $F$, but is not the expected value. What if we receive testable
information about the marginal distribution for $F$?

For any distribution $p(\xx)$, the marginal distribution for $F$ is:
\begin{eqnarray}
p(F) &=& \sum_{\xx}\mathds{1}\left(f(\xx) = F\right)p(\xx)\label{eqn:constraint}
\end{eqnarray}
If $p(F)$ is a specified function, this is an infinite number of expected
value constraints on $p(\xx)$. Therefore the probability distribution
$p(\xx)$ with maximum entropy with respect to $q(\xx)$ that satisfies the
constraint~\ref{eqn:constraint} is:
\begin{eqnarray}
p(\xx) &=& \frac{q(\xx)}{Z}\exp\left[- \right]
\end{eqnarray}



%To keep things concrete, suppose all of the unknown parameters are positive,
%and our prior is an improper uniform distribution:

%\begin{eqnarray}
%q(\boldsymbol{x}) \propto
%\left\{
%\begin{array}{lr}
%1, & \forall x_i > 0\\
%0, & \textnormal{otherwise}.
%\end{array}
%\right.
%\end{eqnarray}

\begin{thebibliography}{}
Jaynes, E. T., 1979, `Where do we Stand on Maximum Entropy?' in The Maximum Entropy Formalism, R. D. Levine and M. Tribus (eds.), M. I. T. Press, Cambridge, MA, p. 15
\end{thebibliography}

\end{document}

